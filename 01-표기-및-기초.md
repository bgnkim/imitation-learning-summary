# 표기법 및 기초 설정

## 필요한 개념

보편적인 개념에 대한 표기

- 1회의 시행에서 흐르는 시간의 index는 $t\in [0, T]$로 표기.
- 각 iteration은 $m$으로 표기하고, iteration마다 사용하는 batch의 크기는 $B$로 표기.
  - 훈련 집합이나 batch에 속한 예시를 나타내는 index는 $i$ ($1\le i\le N$).
  - 훈련 집합은 batch에 무관하게 $D$.
- 모형화나 기계학습에 사용될 $d$ 차원의 매개변수(parameter)는 $w \in \mathbb{R}^d$
- 훈련에 사용할 특성값(feature)은 $\phi$. 대개 각 상태마다 정의되는 $\phi:\mathcal{S}\to \mathbb{R}^d$.
  - $w$와 $\phi$의 index는 $j$ ($1\le j \leq d$).
- 최적해는 superscript \*로 표기 (예: $\pi^*$, $R^*$)



필요한 개념들에 대한 정의를 요약하고, 문서 내부에서 사용할 표기를 정의함.

- **에이전트**(Agent): 복잡한 동적인 환경에서 목표를 달성하려고 시도하는 시스템 [(Wikipedia, 18.11.30)][Agent]

- **전문가**(Expert): 에이전트가 학습할 행동을 수행하는 방법을 알고 있는 사람이나 알고리즘.

  - 전문가의 입력을 나타낼 때는 subscript $E$ 표기 (예: $\pi_E$, $R_E$ )

- **상태**(State): 에이전트가 현재 처한 외부 환경에 대한 정보.

  - 가능한 상태의 집합은 대문자 $\mathcal{S}$. 시작 상태와 종료 상태의 집합은 각각 $\mathcal{S}_I, \mathcal{S}_F\subseteq \mathcal{S}$.
  - 시점 $t$의 상태는 소문자 $s_t$, 가능한 다른 행동들은 $s$

- **행동**(Action): 에이전트가 현재 상태에서 취할 움직임 또는 출력.

  - 상태 $s_t$ 에 따라 정해지는 가능한 행동의 집합은 $\mathcal{A}(s_t)$ 또는 줄여서 $\mathcal{A}$
  - 시점 $t$에 취하는 행동 하나는 $a_t$, 가능한 다른 행동들은 $a$

- **보상**(Reward): 실수 값으로서, 상태를 고려하여 에이전트가 취한 행동에 따른 결과 ([SB2017][]). 에이전트는 보상을 최대화하기 위해 행동함.

  - 비용(cost)은 역보상으로 볼 수 있다.
  - 가능한 보상의 집합은 대문자 $\mathcal{R} \subseteq \mathbb{R}​$. 보통 $[0,1]​$.
  - 시점 $t$에 받은 보상은 $r_t$, 가능한 다른 보상은 $r$. 보상 함수를 표기할 때는 대문자 $R: s\mapsto r\in \mathcal{R}$.
  - 비용함수 또는 손실함수 기준으로 논리가 전개될 때에는 $l$을 사용한다. 예: $l(\pi, \pi_E)$

- **정책**(Policy): 확률분포로서, 에이전트가 각 시점에 행동할 방식을 정의.

  - 일반적인 정책은 $\pi$ 로 표기하며, $\pi : S \to A$ (Deterministic) 또는 $\pi: S \times A \to [0, 1]$ (Stochastic).
  - 시점 $t$에 사용중인 정책은 $\pi_t$
  - 전문가가 사용중인 정책은 $\pi_E$

- $\pi$**의 롤아웃**(Rollout of $\pi$): 초기 상태 $s_0$ 에서 출발하여, 정책 $\pi$를 사용해 앞으로의 행동 및 환경 변화를 시뮬레이션 하는 것.

- **자취**(Trajectory): $\pi$의 롤아웃 결과로 얻어진 상태와 행동의 변화를 나타낸 나열.

  - 일반적인 자취는 $\vec{\tau}$ 또는 $\tau$. $i$번째 자취는 $\vec{\tau}_i$ 또는 $\tau_i$. 이들의 집합은 $\mathcal{T}(\pi)$ 또는 $\pi$가 명시된 경우 줄여서 $\mathcal{T}$.
  - 전문가가 $\pi_E$를 수행(롤아웃)하여 얻은 집합은 **시연**(Demonstration)이며, $\mathcal{T}_E := \mathcal{T}(\pi_E)$ 로 표기.
  - $\tau$는 $S\times(A\times S)^*$의 원소로서, $(s_0, a_0, s_1, a_1, s_2, a_2, \cdots)$와 같이 풀어쓸 수 있음.

- **환경 모형**(State dynamics; Model): 각 상태와 행동마다 주어지는 다음 상태의 확률 분포. 즉, 상태 변화의 확률 분포. $\theta_s$ 또는 $\theta$로 표기
  $$
  \theta(s_t, a_t, s_{t+1}) := P(s_{t+1} | s_t, a_t)
  $$







- **상태의 분포(State distribution)**: $\pi$의 롤아웃 결과로 얻어지는 상태의 출현 확률 분포. $P(s|\pi)$ 또는 $p_\pi$ 로 표기.



------

**References:**

[SB2017]: http://incompleteideas.net/book/bookdraft2017nov5.pdf	"(Sutton & Barto, 2017)​"

- Sutton, R. *Reinforcement Learning: An Introduction*. 2nd ed. 2018

[Agent]: https://ko.wikipedia.org/wiki/지능형_에이전트