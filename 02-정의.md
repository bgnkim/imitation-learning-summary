# 정의

[ICML2018 Tutorial][]에서 주어진 정의를 참고함.

- 주어진 것
  - $\mathcal{T}_E$ (전문가의 시연) 또는 전문가
  - 에이전트가 속할 환경
- 가끔 있는 것 (있을 수도 있고, 없을 수도 있다)
  - 환경 모형
  - 보상
- 하려는 것
  - *목표*: 시연을 모사하기 위한 **정책**의 학습.
  - *입력*: $\mathcal{T}_E$, 다시 말해 $\tau_i$ 들
  - *출력*: $\pi_E$ 보다 비슷하거나 좋은 $\pi$. 또는 그 정책을 나타내는 매개변수 $w$.



## 기타 참고자료에 주어진 정의

[CMU10703][]에서 주어진 정의

> The agent (learner) needs to come up with a policy whose resulting state, action trajectory distribution matches the expert trajectory distribution.

[EACL2017 Tutorial][]의 정의

> Learn policy from expert demonstrations using machine learning algorithms.

[AN2004][]의 정의 "Apprenticeship Learning"

> A task of learning from an expert.

[RBZ2006][]의 정의 "Imitation Learning" 

> A learner attempts to mimic an expert's behavior or control strategy.

[ZMBD2008][]의 정의 "Imitation learning"

> To learn to predict the behavior and decisions an agent would choose - e.g., the motions a person would take to grasp an object or the route a driver would take to get from home to work.



## 왜 지도학습(Supervised Learning)은 안 될까?

*참고한 문서: [ICML2018 Tutorial][] 및 [CMU10703][]*

지도학습이 가지고 있는 일반적인 가정: "훈련 데이터 사이의 **확률적 독립성**"

독립성이 왜? **연속된 판단을 요하는 상황에서는 독립이 아니다!**

- Sequential Decision Making 문제는 이전에 내린 결정에 의해 현재 상황이 변화된다.
- Structured Prediction은 구조적 제한 조건에 따라 이전 예측이 다음 예측의 범위를 제한할 수 있다.
- 인접한, 또는 멀리 떨어진 예측 사이의 정합성은 일반적인 지도학습의 목표가 아니다.

이는 **오류 전파**(Error propagation)의 원인이 된다.

- 오류가 전파되면 학습된 모형은 학습하지 않은 데이터에서 예측을 시도해야 한다.
- 학습되지 않은 곳에서 예측한 것은 다시 오류의 원인이 된다.



다시 말해서, 이런 방법을 사용하려고 할 수 있다:

### Naive approach: Behavioral Cloning

확률적 독립을 가정하고 $\pi$를 찾고자 한다면, 아마도

- 지도학습 알고리즘 중 하나 ($\mathcal{X}$라 하자)를 사용해서
- 현재까지 누적된 상태를 특성값으로 나타내고, (미래를 제외한) 현 시점의 값을 비교하는 loss를 사용해 학습하고,
- 학습 이후에 $\mathcal{X}$가 행동을 판단하게 할 것이다.

'현재까지 누적된 상태'가 이전 과정을 고려하게 하고 싶은 것이지만

- '누적'된 상태를 만드는 주체는 지도학습 알고리즘 $\mathcal{X}$ 가 아니라, 전문가로부터 미리 수집한 학습 집합 $\mathcal{T}_E$이다.
- **다시 말해, $\mathcal{X}$는 자신이 틀릴 경우를 학습해보지 못한다**.

'현재' 이 순간의 loss를 고려하는 것은, 문제를 단순화하기 위한 모형화지만

- '현재'가 맞더라도 전체가 틀릴 수 있다 (Viterbi와 Forward 알고리즘을 생각해보라)
- **다시 말해, 전체적 구조를 고려한 예측은 아니다.**



그래서

1. $\mathcal{X}$가 **경험하지 못한 상태**를 어떻게 처리할 것인지를 우선 생각해볼 수 있다. → [Direct Policy Learning](./03-방법론/DirectPolicyLearning.md)
2. 아니면 $\mathcal{X}$ 가 좀 더 **일반적인 계획을 할 수 있도록** 생각해볼 수 있다. → [Inverse Reinforcement Learning](./03-방법론/InverseReinforcementLearning.md)



### Imitation Learning의 강점 (정리 전)

[SBS2008][]:

> The first is that reward functions are often difficult to describe exactly, and yet at the same time it is usually easy to specify what the rewards must depend on. ... The second observation is that demonstrations of good policies by experts are often plentiful.



------

**References:**

[ICML2018 Tutorial]: https://sites.google.com/view/icml2018-imitation-learning/	" Imitation Learning Tutorial (Retrieved at 18.11.30) "
[CMU10703]: http://www.andrew.cmu.edu/course/10-703/	"Deep Reinforcement Learning and Control (Retrieved at 18.11.30)"
[EACL2017 Tutorial]: https://github.com/sheffieldnlp/ImitationLearningTutorialEACL2017	" EACL2017 Tutorial on Imitation Learning "
[AN2004]: http://doi.acm.org/10.1145/1015330.1015430	" Abbeel & Ng. (2004). Apprenticeship Learning via Inverse Reinforcement Learning. ICML '04 "
[RBZ2006]: http://doi.acm.org/10.1145/1143844.1143936	" Ratliff, Bagnell, & Zinkevich. (2006). Maximum Margin Planning. ICML '06 "
[ZMBD2008]: https://dl.acm.org/citation.cfm?id=1620297	" Ziebart et al. (2008). Maximum entropy inverse reinforcement learning. AAAI '08 "

[SBS2008]: "Syed et al. (2008). Apprenticeship learning using linear programming, ICML08"