# Imitation Learning

* 이 문서는 개인적인 정리 결과물이며 신뢰성을 담보할 수 없으니 논문을 직접 읽는 것을 권한다.
* 이 문서는 수식이 많으므로 Typora로 보는 것이 좋다.

[TOC]



## 표기법 및 기초 설정

### 보편적인 개념에 대한 표기

- 1회의 시행에서 흐르는 시간의 index는 $t\in [0, T]$로 표기.
- 각 iteration은 $m$으로 표기하고, iteration마다 사용하는 batch의 크기는 $B$로 표기.
  - 훈련 집합이나 batch에 속한 예시를 나타내는 index는 $i$ ($1\le i\le N$).
  - 훈련 집합은 batch에 무관하게 $D$.
- 모형화나 기계학습에 사용될 $d$ 차원의 매개변수(parameter)는 $w \in \mathbb{R}^d$
- 훈련에 사용할 특성값(feature)은 $\phi​$. 대개 각 상태마다 정의되는 $\phi:\mathcal{S}\to \mathbb{R}^d​$.
  - $w​$와 $\phi​$의 index는 $j​$ ($1\le j \leq d​$).
- 최적해는 superscript \*로 표기 (예: $\pi^*​$, $R^*​$)



### 필요한 개념의 정의

필요한 개념들에 대한 정의를 요약하고, 문서 내부에서 사용할 표기를 정의함.

- **에이전트**(Agent): 복잡한 동적인 환경에서 목표를 달성하려고 시도하는 시스템 ([Wiki:Agent][])

- **전문가**(Expert): 에이전트가 학습할 행동을 수행하는 방법을 알고 있는 사람이나 알고리즘.

  - 전문가의 입력을 나타낼 때는 subscript $E$ 표기 (예: $\pi_E$, $R_E$ )

- **상태**(State): 에이전트가 현재 처한 외부 환경에 대한 정보.
  - 가능한 상태의 집합은 대문자 $\mathcal{S}$. 시작 상태와 종료 상태의 집합은 각각 $\mathcal{S}_I, \mathcal{S}_F\subseteq \mathcal{S}$.
  - 시점 $t$의 상태는 소문자 $s_t$, 가능한 다른 행동들은 $s$

- **행동**(Action): 에이전트가 현재 상태에서 취할 움직임 또는 출력.

  - 상태 $s_t$ 에 따라 정해지는 가능한 행동의 집합은 $\mathcal{A}(s_t)$ 또는 줄여서 $\mathcal{A}$
  - 시점 $t$에 취하는 행동 하나는 $a_t$, 가능한 다른 행동들은 $a$

- **보상**(Reward): 실수 값으로서, 상태를 고려하여 에이전트가 취한 행동에 따른 결과 ([Sutton & Barto, 2017][]). 에이전트는 보상을 최대화하기 위해 행동함.

  - 비용(cost)은 역보상으로 볼 수 있다.
  - 가능한 보상의 집합은 대문자 $\mathcal{R} \subseteq \mathbb{R}$. 보통 $[0,1]$.
  - 시점 $t$에 받은 보상은 $r_t$, 가능한 다른 보상은 $r$. 보상 함수를 표기할 때는 대문자 $R: s\mapsto r\in \mathcal{R}$.
  - 비용함수 또는 손실함수 기준으로 논리가 전개될 때에는 $l$을 사용한다. 예: $l(\pi, \pi_E)$
  - 보상은 미래의 것이므로, 보통 할인율(discount rate) $\gamma$를 적용하여 현재의 보상으로 계산하기도 한다.

- **정책**(Policy): 확률분포로서, 에이전트가 각 시점에 행동할 방식을 정의.

  - 일반적인 정책은 $\pi$ 로 표기하며, $\pi : S \to A$ (Deterministic) 또는 $\pi: S \times A \to [0, 1]$ (Stochastic).
  - 시점 $t$에 사용중인 정책은 $\pi_t$
  - 전문가가 사용중인 정책은 $\pi_E$

- $\pi$**의 롤아웃**(Rollout of $\pi$): 초기 상태 $s_0$ 에서 출발하여, 정책 $\pi$를 사용해 앞으로의 행동 및 환경 변화를 시뮬레이션 하는 것.

- **자취**(Trajectory): $\pi$의 롤아웃 결과로 얻어진 상태와 행동의 변화를 나타낸 나열.

  - 일반적인 자취는 $\vec{\tau}$ 또는 $\tau$. $i$번째 자취는 $\vec{\tau}_i$ 또는 $\tau_i$. 이들의 집합은 $\mathcal{T}(\pi)$ 또는 $\pi$가 명시된 경우 줄여서 $\mathcal{T}$.
  - 전문가가 $\pi_E$를 수행(롤아웃)하여 얻은 집합은 **시연**(Demonstration)이며, $\mathcal{T}_E := \mathcal{T}(\pi_E)$ 로 표기.
  - $\tau$는 $S\times(A\times S)^*$의 원소로서, $(s_0, a_0, s_1, a_1, s_2, a_2, \cdots)$와 같이 풀어쓸 수 있음.

- **환경 모형**(State dynamics; Model): 각 상태와 행동마다 주어지는 다음 상태의 확률 분포. 즉, 상태 변화의 확률 분포. $\theta_s$ 또는 $\theta$로 표기
  $$ \theta(s_t, a_t, s_{t+1}) := P(s_{t+1} | s_t, a_t)$$


- **상태의 분포(State distribution)**: $\pi$의 롤아웃 결과로 얻어지는 상태의 출현 확률 분포. $P(s|\pi)$ 또는 $p_\pi$ 로 표기.



------

## 정의

[ICML2018 Tutorial][]에서 주어진 정의를 참고함.

- 주어진 것
  - $\mathcal{T}_E$ (전문가의 시연) 또는 전문가
  - 에이전트가 속할 환경
- 가끔 있는 것 (있을 수도 있고, 없을 수도 있다)
  - 환경 모형
  - 보상
- 하려는 것
  - *목표*: 시연을 모사하기 위한 **정책**의 학습.
  - *입력*: $\mathcal{T}_E$, 다시 말해 $\tau_i$ 들
  - *출력*: $\pi_E$ 보다 비슷하거나 좋은 $\pi$. 또는 그 정책을 나타내는 매개변수 $w$.



### 기타 참고자료에 주어진 정의

[CMU10703][]에서 주어진 정의

> The agent (learner) needs to come up with a policy whose resulting state, action trajectory distribution matches the expert trajectory distribution.

[EACL2017 Tutorial][]의 정의

> Learn policy from expert demonstrations using machine learning algorithms.

[Abbeel & Ng, 2004][]의 정의 "Apprenticeship Learning"

> A task of learning from an expert.

[Ratliff et al., 2006][]의 정의 "Imitation Learning" 

> A learner attempts to mimic an expert's behavior or control strategy.

[Ziebart et al., 2008][]의 정의 "Imitation learning"

> To learn to predict the behavior and decisions an agent would choose - e.g., the motions a person would take to grasp an object or the route a driver would take to get from home to work.



### 왜 지도학습(Supervised Learning)은 안 될까?

*참고한 문서: [ICML2018 Tutorial][] 및 [CMU10703][]*

지도학습이 가지고 있는 일반적인 가정: "훈련 데이터 사이의 **확률적 독립성**"

독립성이 왜? **연속된 판단을 요하는 상황에서는 독립이 아니다!**

- Sequential Decision Making 문제는 이전에 내린 결정에 의해 현재 상황이 변화된다.
- Structured Prediction은 구조적 제한 조건에 따라 이전 예측이 다음 예측의 범위를 제한할 수 있다.
- 인접한, 또는 멀리 떨어진 예측 사이의 정합성은 일반적인 지도학습의 목표가 아니다.

이는 **오류 전파**(Error propagation)의 원인이 된다.

- 오류가 전파되면 학습된 모형은 학습하지 않은 데이터에서 예측을 시도해야 한다.
- 학습되지 않은 곳에서 예측한 것은 다시 오류의 원인이 된다.



다시 말해서, 이런 방법을 사용하려고 할 수 있다:

#### Naive approach: Behavioral Cloning

확률적 독립을 가정하고 $\pi$를 찾고자 한다면, 아마도

- 지도학습 알고리즘 중 하나 ($\mathcal{X}$라 하자)를 사용해서
- 현재까지 누적된 상태를 특성값으로 나타내고, (미래를 제외한) 현 시점의 값을 비교하는 loss를 사용해 학습하고,
- 학습 이후에 $\mathcal{X}$가 행동을 판단하게 할 것이다.

'현재까지 누적된 상태'가 이전 과정을 고려하게 하고 싶은 것이지만

- '누적'된 상태를 만드는 주체는 지도학습 알고리즘 $\mathcal{X}$ 가 아니라, 전문가로부터 미리 수집한 학습 집합 $\mathcal{T}_E$이다.
- **다시 말해, $\mathcal{X}$는 자신이 틀릴 경우를 학습해보지 못한다**.

'현재' 이 순간의 loss를 고려하는 것은, 문제를 단순화하기 위한 모형화지만

- '현재'가 맞더라도 전체가 틀릴 수 있다 (Viterbi와 Forward 알고리즘을 생각해보라)
- **다시 말해, 전체적 구조를 고려한 예측은 아니다.**



그래서

1. $\mathcal{X}$가 **경험하지 못한 상태**를 어떻게 처리할 것인지를 우선 생각해볼 수 있다. → [Direct Policy Learning](#direct-policy-learning)
2. 아니면 $\mathcal{X}$ 가 좀 더 **일반적인 계획을 할 수 있도록** 생각해볼 수 있다. → [Inverse Reinforcement Learning](#inverse-reinforcement-learning-(with-model))



### Imitation Learning의 강점

[Syed et al., 2008][]은 다음과 같이 말했다.

> The first is that reward functions are **often difficult to describe exactly**, and yet at the same time it is usually easy to specify what the rewards must depend on. ... The second observation is that **demonstrations of good policies by experts are often plentiful**.

[Clark & Manning, 2015][]는 다음과 같이 말한다.

> We face a sequential prediction problem where future observations (visited states) depend on previous actions. **This is challenging because it violates the common i.i.d. assumptions** made in statistical learning. **Imitation learning**, where expert demonstrations of good behavior are used to teach the agent **has proven very useful in practice for this sort of problem** (Argall et al., 2009).



------

# 방법론

## Direct Policy Learning

*참조: [ICML2018 Tutorial][]*

알고리즘이 **경험하지 못한 상태**를 어떻게 처리할 것인지를 생각해보면, 가장 간단한 답은:

- 전문가에게 묻는 것
- 다른 알고리즘이 어떻게 하는지 보는 것

이다. 즉, **누가 어떻게 하는지 보는 것**이다.



<u>[전제조건]</u> 

- 학습하는 동안 사람이나 기타 전문가에게 실시간으로 물어볼 수 있다.
- 또는, 학습하기 위해서 임의의 상태에서 학습을 시작하거나, 학습과 시뮬레이션을 자유롭게 오갈 수 있다.
  - 예: 언어처리에서 품사 분석 과정은 문장 중간에서 시작할 수 있다.



[<u>일반적인 방법</u>]

1. 초기 정책 $\pi_0$를 수립한다.
2. $m$을 증가시키면서 $\pi_{m+1}$을 $\pi_m$으로부터 다음과 같이 구성한다:
   1. $\pi_m$의 롤아웃 과정을 통해 $\mathcal{T}$를 구성한다.
   2. $\mathcal{T}$으로부터 상태 $s\in S$의 출현 확률 분포 $p_{\pi_m}$을 구성한다.
   3. 전문가나 학습 데이터를 참조해서 각 상태에서의 정답 행동 $\mathcal{T}_E$를 구성한다.
   4. $\mathcal{T}_E$, $p_{\pi_m}$을 활용해서 $\pi_{m+1}$을 갱신한다.



어떻게 업데이트 해야 할까?

Naive Approach: $m$만으로 $m+1$ 만들기

$\mathcal{T}_E$과 $p_{\pi_m}$만으로 $\pi_{m+1}$을  만들 수 있지 않을까? 즉, $\lim_{m\to\infty} \pi_m \simeq \pi_E$ 아닐까?

- 일단 알고리즘이 경험하지 못한 상태는 $\mathcal{T}_E$로 보완할 수 있게 되었다.
- 그러면, $\mathcal{T}_E$가 일관되어야, 즉 $\pi_0, \pi_1, \cdots, \pi_m$ 아래에서 상태의 출현 분포가 비슷해야 한다.
- 최소한, $p_{\pi_m}(s) > 0$이라면 $(\forall i>m)\ [p_{\pi_i}(s)>0]$ 이어야 한다.



**근데, 그렇지 않다는게 문제다**.

- 매 반복마다 $\pi_m$ 이 변하고 있으므로, 이전에 방문한 상태 $s$를 다시 방문하지 않을수도 있다.
- 방문하지 않은 상태에 대한 정책은 이전 정책을 유지하지 않을 가능성이 높다.
- 극단적으로, $\pi_m$은 수렴하지 않고 진동할 가능성이 있다: 방문했다가, 방문하지 않았다가, ...



그래서, 이전에 방문했었던 상태의 정책을 기억해야 한다.

### Stochastic Mixing: 정책 누적하기

정책은 확률분포이므로, 확률분포를 섞듯이 섞어볼 수 있다.



#### [Daume et al., 2009][] (SEARN)

1. $\pi_E$로 $\pi_0$를 초기화한다.
2. $\pi_m$ 이 $\pi_E$ 에 비해서 잘 하지 못하는 동안^1^ , 다음을 반복한다:
   1. 훈련 집합 $D = \emptyset$ 으로 초기화한다.
   2. $\mathcal{T}_E$ 에 포함된 전문가의 행동열 $\tau_i=(s_0, a_0, s_1, a_1, \cdots)$에 대해:
      1. $\pi_m$을 $s_0, s_1, \cdots, s_T$ 상태에서 롤아웃하여 $\hat{\tau}_i$를 얻는다^2^.
      2. 각 시간 $t$마다:
         1. $\phi_t := \phi (\mathbf{s}, a_0, a_1, \cdots, a_t)$를 계산한다.
         2. $\pi_m$의 행동을 선택하지 않고 다른 행동 $a$ 를 선택하는 경우를 생각하기 위해,
            행동을 $a$로 교체한 $\pi_{m,a}$를 생각하고 $a$를 선택할 때의 보상 $r_a$ 를 $\min_{a'} \mathbb{E}_{\tau \sim p_{\pi_{m,a'}}} l(\tau, \tau_i) - \mathbb{E}_{\tau\sim p_{\pi_{m,a}}} l(\tau, \tau_i) $ 로 계산한다.
      3. $D$ 에 $(\phi, \mathbf{r})$를 추가한다.
   3. $D$를 학습데이터로 하여 적당한 지도학습 알고리즘으로 $\hat{\pi}_{m+1}$을 훈련한다.
   4. $\pi_{m+1}=\beta\hat{\pi}_{m+1}+(1-\beta)\pi_m$ 으로 업데이트한다. **(정책을 섞는다)**
3. $\tilde{\pi} = \frac{\pi_M - (1-\beta)^M\pi_E}{1-(1-\beta)^M}$으로 지정해서 $\pi_E$의 효과를 제거한다.



주^1^: 상수 $C$와 $\beta$ 에 대해서, $m\le C/\beta$ 인 동안이면 충분하다. (논문에 $C$가 무엇인지는 안 적혀있다.)

주^2^: SEARN은 고정된 입력에 labeling을 하는 task를 수행하므로, $s_0, s_1, \cdots, s_T$는 $\pi$에 무관하게 고정이다.



#### [Ross & Bagnell, 2010][] (SMILe)

1. $\pi^*$로 $\pi_0$를 초기화한다.
2. 적당한 횟수 $M$동안^3^ 다음을 반복한다:
   1. $\pi_m$을 롤아웃하여 크기 $B$인 $\mathcal{T}$을 얻는다.
   2. $\mathcal{T}$에 나타나는 상태분포를 기준으로 전문가에게 물어서 $\mathcal{T}_E$를 만든다.
   3. $\mathcal{T}_E$ 를 학습데이터로 하여 적당한 지도학습 알고리즘으로 $\hat{\pi}_{m+1}$을 훈련한다.
   4. $\pi_{m+1} = \beta \hat{\pi}_{m+1} + (1-\beta) \pi_m$로 업데이트한다. **(정책을 섞는다)**
3. $\tilde{\pi} = \frac{\pi_M - (1-\beta)^M\pi_E}{1-(1-\beta)^M}$으로 지정해서 $\pi_E$의 효과를 제거한다.



주^3^: $\alpha = \frac{\sqrt{3}}{T^2\sqrt{\log T}}$, $M=\frac{2\log T}{\alpha}$ 일 때, Regret $T[\mathbb{E}_{s\sim P(s|\tilde{\pi})}l(\pi^*, \tilde{\pi})-\min_{\pi'}\mathbb{E}_{s\sim P(s|\pi')}l(\pi^*, \pi')]$가 $T^2$에 비례하게 된다. (Regret의 상한)



#### SEARN, SMILe은 어떤 특징이 있나?

- Imitation learning을 Stochastic한 지도학습의 순서열로 환원(reduction)시켰다.
- 수렴성이 **보장**된다. 그러나 오차가 0이 되는 것은 아니다. ($T^2$에 비례한다)
- **부정확한 정책**을 지속적으로 시도해야 한다. 시도할 때마다 지속적으로 Expert의 정책을 알아야 한다.



**문제는...**

- $T$가 크면 잘 돌지 않게 된다 (오류도 $T^2$에 비례하지만 수행 시간도 만만치 않다)
- 계속해서 혼합되는 $\pi_m$이 다른 것들에 비해 나빠도 제거할 방법이 없다.



### Data Aggregation: 상태 누적하기

그래서, 결과가 아닌 결과의 재료를 모아보기로 한다.



#### [Ross et al., 2011][] (DAgger)

1. $D$ 를 $\emptyset$으로 초기화한다.
2. $\hat{\pi}_0$를 임의로 선택한다. (전문가 정책일 필요 없다)
3. 적당한 횟수 $M$ 동안^4^ 반복한다:
   1. $\pi_m := \beta_m\pi_E +(1-\beta_m)\hat{\pi}_m$으로 롤아웃하여 크기 $B$인 $\mathcal{T}$을 만든다.
   2. $\mathcal{T}$에 나타나는 상태분포를 기준으로 전문가에게 물어서 $\mathcal{T}_E^{(m)} := \{(s, \pi^*(s)) | s\in\mathcal{T}(\pi_m)\}$을 구성한다.
   3. $D := D \cup \mathcal{T}_E^{(m)}$으로 갱신한다. **(상태 분포를 누적한다)**
   4. $D$를 훈련집합으로 하여 적당한 지도학습 알고리즘을 사용해 $\hat{\pi}_{m+1}$을 훈련한다.
4. 별도로 구성된 검증 집합을 기준으로, 가장 좋은 성능을 보이는 $\hat{\pi}_m$을 선택한다.



주^4^: 일반화 오류를 줄여서 최적해를 찾을 확률이 $1-\delta$가 되려면, $BM=O(-T^2\log\delta)$ 로 설정하면 된다.
(Strong convex loss인 경우 $O(T\log(T/\delta))$).



#### DAgger는 어떤 특징이 있나?

- (SEARN, SMILe과 같이) 역시 수렴성이 보장되고, 부정확한 정책을 시도하며, 지속적으로 전문가에게 물어야 한다.
- Imitation Learning을 No-regret Online Learning으로 환원하여 오차를 줄였다.
  (즉, 오차 $\mathbb{E}_{s\sim p_\tilde{\pi}}l(\pi^*, \tilde{\pi})-\min_{\pi'}\mathbb{E}_{s\sim p_{\pi'}}l(\pi^*, \pi')$ 가 0에 가까워지는 Online learning이다)



**그러나 여전히 문제는...**

- 부정확한 정책을 실행할 수 있을 때만 사용가능하다.
- 지속적으로 전문가에게 물을 수 있고, 물어볼 방법이 있어야 한다.
- 전문가가 모든것을 정확히 안다고 가정한다. 즉, suboptimal 전문가를 고려하지 않는다.
- 전체 작업의 보상이 각 행동의 보상으로 균등분할되지 않는다.



그래서... 시스템이 **혼자서 할 수 있으면** 좋겠다.

- 시뮬레이션을 통해 상태 변화를 생각해보게 하자.
- 전문가에게는 가끔만 물어보자.
- 전문가보다 나은 정책을 찾아보자.
- 보상을 어떻게 나눌지 모르겠으니 알아서 보상을 얻게 하자.

→ [Inverse Reinforcement Learning (With Model)](#inverse-reinforcement-learning-(with-model))



------

## Inverse Reinforcement Learning (WITH Model)

*참조: [ICML2018 Tutorial][]*

시스템이 스스로 하게 하는 것은,

- 시스템이 목표를 좇아 효율적으로 행동하게 하는 것이고
- 다시 말해, 시스템이 보상을 최대화하는 방향으로 행동하게 하는 것이며

따라서 **강화학습**과 연결된다.

다만, 보통의 경우 전문가는 행동의 목표를 알지만 그것을 수식으로 적지는 못하는 상황이다.



<u>[전제조건]</u>

- $S$, $A$, $\pi^*$는 주어진다.
- 환경 모형 $\theta$는 일단은 알고 있다고 전제하자.



<u>[할 일]</u>

- 알지 못하는 보상함수 $R^*$를 찾아 최종적으로는 전문가의 행동을 따라하거나, 더 나은 행동을 해야 한다. 다시 말해 
  $$
  \pi^* = \arg\max_{\pi} \mathbb{E}_{s\sim p_\pi} \left[ R^*(s) | \pi \right]
  $$
  가 성립하는 $R^*$를 찾아야 한다. 또는 Value 함수로 표기하면 아래와 같다. 
  $$
  \mathbb{E}\left[\left.\sum_t \gamma^t R^*(s) \right| \pi^\star \right] \ge \mathbb{E}\left[\left.\sum_t \gamma^t R^*(s) \right| \pi \right] \quad\forall\pi
  $$






### IRL/IOC의 정의

이렇게 보상함수를 찾아 학습하는 방법을 Inverse Reinforcement Learning 또는 Inverse Optimal Control이라 한다.

[Abbeel & Ng, 2004][]가 따르는 정의 "Inverse Reinforcement Learning"

> The problem of deriving a reward function from observed behavior is referred to as inverse reinforcement learning (Ng & Russell, 2000)

[Ratliff et al., 2006][]에서 언급한 정의 "Inverse Reinforcement Learning"

> The goal in IRL is to observe an agent acting in an MDP (with unknown reward function) and extract from the agent's behavior the reinforcement function it is attempting to optimize.

[Ziebart et al., 2008][]이 따르는 정의 "Inverse Reinforcement Learning"

> ... to recovering a reward function that induces the demonstrated behavior with the search algorithm serving to "stitch-together" long, coherent sequences of decisions that optimize that reward function.

[Finn et al., 2016][]이 따르는 정의

> Inverse optimal control (IOC) or inverse reinforcement learning (IRL) provide an avenue for addressing this challenge[defining a cost function] by learning a cost function directly from expert demonstrations, e.g. ...



<u>[일반적인 방법]</u>

1. $\mathcal{T}_E$를 사용해서 $R_w$를 학습 (보통 $R_w = w^\top \phi(s, a)$로 가정)
2. 학습된 $R_w$와 주어진 데이터를 사용해서 $\pi$ 학습 (강화학습 알고리즘 사용)
3. $\pi$와 $\pi_E$를 비교하여 차이가 많이 난다면 1부터 반복.

문제는, 같은 목표를 표현하는 **$R_w$가 여러개**라는 것. 어떤 기준으로 여러 개 중 하나를 골라야 할까?



### New Concept: Feature Expectation

$R_w = w^\top \phi(s, a)$ 라고 가정하면, 다음이 성립한다.
$$
\mathbb{E}\left[\left. \sum_t \gamma^t R_w(s) \right| \pi \right] = \mathbb{E}\left[\left. \sum_t \gamma^tw^\top\phi(s) \right| \pi\right] = w^\top\cdot\mathbb{E}\left[\left.\sum_t \gamma^t\phi(s)\right| \pi\right]
$$
$ \mathbb{E}[\sum_t \gamma^t\phi(s)|\pi]$ 는 $\pi$에 따라 변화하며, 이외의 변인은 없다.

따라서, 이를 **feature expectation**이라 하고, $\mu(\pi)$ 또는 $\mu$로 표기한다.



Feature expectation을 활용해 식 (2)를 다시 서술하면 다음과 같다.
$$
w^\top \cdot \mu(\pi^*) \ge w^\top \cdot \mu(\pi)\quad \forall\pi
$$
꼭 어디서 많이 본 문제 같다.



### Maximum Margin Methods

Support vector machine의 기초가 되는 Maximum margin classifier는 보통 다음 최대화 문제의 해인 $w$를 찾는다.
$$
\max_{\zeta, w}\quad \zeta\\
\textrm{s.t.}\quad w^\top x^+ \ge w^\top x^- + \zeta\quad \forall x^+ \forall x^-\\
\quad \|w\|_2 \le 1
$$
식 (4)는 식 (5)와 닮았다. $\mu_E:=\mu(\pi_E)$가 유일한 positive example이라고 두면, 다음과 같이 Maximum margin 형태로 만들어 볼 수 있다.



#### [Abbeel & Ng, 2004][] (QP version)

$\epsilon$은 허용할 오차 범위를 나타내며, $\mu_E$는 $\mathcal{T}_E$를 표본으로 하여 근사할 수 있다.

1. 임의로 $\pi_0$를 선택한다.
2. $\zeta_m > \epsilon$인 동안 $m=1,2,\cdots$을 증가시키면서 반복한다:
   1. $\mu(\pi_m)$을 계산한다  (실제 가능한 경우를 모두 구하든, Monte Carlo 샘플링을 하든...)
   2. **(보상함수의 추정)** $\zeta_m\triangleq \max_{w:\|w\|_2<1} \min_{n: 0\le n<m} w^\top (\mu_E - \mu(\pi_n))$를 이차계획법으로 풀어낸다.
      이 때, $w_m$은 해당 $\zeta_m$을 만드는 $w$값이 된다.
   3. **(최적 정책 찾기)** 강화학습 알고리즘을 사용해 보상함수가 $R_{w_m}$일때의 최적 정책 $\pi_m$을 찾는다.
3. $\{\pi_m | m = 0, 1, \cdots\}$ 를 반환한다.



- 이 절차는 최대 $M = O(\frac{d}{(1-\gamma)^2\epsilon^2}\log\frac{d}{(1-\gamma)\epsilon})$ 만큼의 반복수에 종료된다.
- $1-\delta$ 만큼의 확률로 최대 $M$회의 반복수로 종료되려면, 아래 조건으로 충분하다:
  $$ |\mathcal{T}_E| \ge \frac{2d}{(\epsilon(1-\gamma))^2}\log\frac{2d}{\delta}. $$



#### Abbeel & Ng, 2004 (Projection version)

위의 QP version에서 Step 2-2를 다음과 같이 변경한다.

1. $m=1$이라면 $w_1=\mu_E - \mu(\pi_0)$ 이고 $\zeta_1 = \|w_1\|_2$ 이다. 아래는 $m>1$일 때를 나타낸다.
2. $\bar{\mu}_{m-1} \triangleq \bar{\mu}_{m-2} + \frac{(\mu_{m-1} - \bar\mu_{m-2})^\top (\mu_E - \bar{\mu}_{m-2})}{(\mu_{m-1} - \bar\mu_{m-2})^\top(\mu_{m-1}-\bar\mu_{m-2})} (\mu_{m-1}-\bar\mu_{m-2})$와 같이 계산하여, $\mu_E$를 선분 $\bar\mu_{m-2}\mu_{m-1}$에 사영시킨다. (단, $\bar\mu_0 = \mu(\pi_0)$)
3. $w_m = \mu_E-\bar\mu_{m-1}$ 로 설정하여 weight를 업데이트한다.
4. $\zeta_m = \|w_m\|_2$로 계산한다.



- QP Version과 동일한 성질을 갖는다.



#### [Ratliff et al. 2006][] (MMP)

이 방법은 여러개의 MDP가 있다고 가정한다. $k$를 각 MDP를 구분하는 index로 사용하자.

다음과 같이 multiple MDP 상황으로 확장할 수 있다 ([CMU10703][] 참고):

1. 각 MDP의 해집합 $\Pi_k$를 공집합으로 초기화한다.

2. 다음 최대화 문제를 풀어서 $w$를 구한다.
   $$ \min_w\quad \|w\|_2^2 + C\sum_k \zeta_k\\
   \textrm{s.t.}\quad w^\top \mu(\pi_{E,k}) \ge w^\top \mu(\pi_k) + l(\pi_{E, k}, \pi_k)-\zeta_k\quad \forall k, \pi_k\in\Pi_k. $$

3. 각 $k$ 마다:

   1. 찾은 $w$를 사용하여, 각 $k$마다 가장 크게 위반된 조건^1^을 나타내는 $\pi_k$를 찾는다^2^:

   $$ \pi_k = \arg\max_{\pi_k} w^\top \mu(\pi_k) + l(\pi_{E,k}, \pi_k) $$

   1. $\pi_k$를 $\Pi_k$에 더한다.

4. 만약 새로 더해지는 $\pi_k$가 없었다면, $\Pi_k$ 들을 반환한다.



주^1^: SVM의 Support Vector와 비슷하다.

주^2^: 환경 모형을 안다면 계산하도록 하고, 모른다면 강화학습을 사용한다.



#### [Syed & Schapire, 2007][] (MWAL)

게임이론의 접근법을 사용해서 $w$를 찾는 min player와 $\pi$를 찾는 max player의 평형점을 찾도록 접근한다. 다시말해, 다음 문제를 풀고자 노력한다:
$$
V_k(\pi) \triangleq \mathbb{E}\left[\sum_{t=0}^\infty \gamma^t \phi_k (s, a)|\alpha, \pi,\theta\right]\\
\max_\pi \min_k \quad V_k(\pi) - V_k(\pi_E) \quad\textrm{s.t.}\\
\|w\|_1 = 1, (\forall k) [w_k \ge 0].
$$
식의 해를 각각 $v^*$와 $\pi_A$라고 하자. 

$V_k$의 정의가 $\mu$ 와 유사하므로, feature expectation의 차이, 즉 $\mu - \mu_E$를 최적화하는 문제와 같다.

따라서, 게임 행렬 $G$를 모든 $\pi$ 값에 대한 $V_k$의 차이로 두자: $G(m, j) \triangleq \mu(\pi_m)(j) - \mu_E(j)$.

알고리즘은 다음과 같다.

1. $\beta = \left( 1+\sqrt{\frac{2\ln k}{T}} \right)^{-1}$.
2. 모든 $\vec{\mu}\in\mathbb{R}^d$에 대해 $\tilde G: (\vec{\mu}) \mapsto \frac{1-\gamma}{4} (\vec{\mu} - \vec{\hat{\mu}}_E  \oplus 2)$로 정의한다 ($\oplus$는 element-wise addition).
   $\vec{w}_1 = [1, 1, 1, \cdots, 1]\in \mathbb{R}^d$로 설정한다.
3. $m=1, \cdots, M$까지의 iteration 동안:
   1. $\mathbf{w}_m = \frac{1}{\|\vec{w}_m\|_1}\vec{w}_m$.
   2. $R_{\mathbf{w}_m}$를 보상함수로 하여 최적 정책 $\hat\pi_m$ 를 $\epsilon_P$ 오차 이내로 추정한다.
   3. 찾은 최적 정책을 바탕으로 $\mathbf{\mu}(\hat\pi_m)$을 $\epsilon_F$ 오차 이내로 추정하여 $\hat\mu_m$이라 한다.
   4. $\vec{w}_{m+1} = \vec{w}_m\odot \beta e^{\tilde{G}(\hat{\mu}_m)}$으로 설정한다. ($\odot$는 element-wise multiplication, 지수 연산도 element-wise.)
4. $\{\hat\pi_m | m=1, 2, \cdots, M\}$ 을 반환한다. (또는 이들이 균등하게 섞인 정책을 반환한다.)



- 결과로 얻은 정책은 $\pi_E$보다 좋을 수도 있다([Abbeel & Ng, 2004][]나 [Ratliff et al., 2006][]과 달리 상한이 존재하지 않는다.)
- 결과로 얻은 정책이 $\pi_E$보다 $\epsilon$ 오차 범위 이내에서 좋을 확률이 $1-\delta$이길 원한다면, 다음으로 충분하다 (정리의 상세한 형태는 [Syed & Schapire, 2007][] 참고).
  $$ M \ge \frac{9\ln d}{2(\epsilon'(1-\gamma))^2}, |\mathcal{T}_E| \ge \frac{2}{(\epsilon'(1-\gamma))^2}\ln\frac{2d}{\delta}$$




#### Maximum Margin의 문제

Maximum Margin은 정규화(regularization)를 통해 $\pi$가 한정되게 하여 여러 가능한 정책 중에 하나를 고르는 문제를 해결하려 했지만. 여전히, 

- 거의 모든 행동과 합치되는 보상함수나 정책은 없다.
- Feature expectation이 동일하도록 만들 수 있는 **정책들의 조합이 너무 많다**. (정책을 확률적으로 조합하면 만들 수 있다.)
  - 만약 $\pi_1$과 $\pi_2$가 $\mu(\pi_1) \simeq \mu_E$, $\mu(\pi_2)\simeq \mu_E$ 였다면,
    임의의 $\alpha$에 대해 $\mu(\alpha\pi_1 + (1-\alpha)\pi_2)\simeq\mu_E$도 성립한다.
  - Sub-optimal Expert의 정책을 사용하거나, 알고리즘이 관련된 상태공간을 모두 고려하지 못할 때 많이 발생한다. ([Ziebart et al., 2008][])



### Maximum Entropy Methods

다시, $R$을 모르는데 $R$을 극대화하는 $\pi$를 찾아야 하는 문제가 되었다.

전문가를 살펴보면, 전문가는 보상을 극대화하는 자취를 선호한다. 다시 말해서, 자취의 분포는 보상과 연관이 있다.

그러면, 간단하게 softmax 형태를 취해서 이렇게 생각해 볼 수 있을것이다.
$$
R(\tau) = \sum_{s_j} w^\top \phi(s)\\
P(\tau) \propto e^{R(\tau)}
$$
좋은 확률분포는, Entropy가 최대인 확률분포이다.

즉, $w = \arg\max_w \sum_i \log P(\tau)$를 Gradient ascent로 풀어보자. 
$$
L(w) := \sum_{\tau\in\mathcal{T}_E} \log P(\tau|w)\\
\Rightarrow \nabla L(w) = \frac{1}{N}\sum_i \phi(\tau_i) - \sum_\tau P(\tau|w)\phi(\tau) = \frac{1}{N}\sum_i \phi(\tau_i) - \sum_s D_s \phi(s)
$$
이 때, $D_s$는 $s$를 방문하는 빈도 값이다. 이를 활용해서 다음과 같이 $w$를 계산할 수 있다.



#### [Ziebart et al., 2008][] (MaxEntIRL)

**Backward pass** 

$\sum P(\tau) = 1$을 만들기 위해서, 총합을 계산하는 과정으로, 계산하기 쉬운 $\mathcal{S}_F$부터 역으로 연산함.

1. 모든 $s\in\mathcal{S}_F$에 대해 $Z_s = 1$로 설정한다.

2. Time horizon만큼 다음을 반복해서 계산한다.
   $$ Z_{a_{i,j}} = \sum_k \theta(s_i, a_{i,j}, s_k)e^{R(s_i|w)}Z_{s_k}\\
   Z_{s_i} = \sum_{a_{i,j}} Z_{a_{i,j}} + \mathbb{I}(s_i\in F) $$


**Local action probability computation**

각 상태별로 행동을 취할 확률을 계산함. $s_i$를 지나가는 경로 중에서 $a_{i,j}$를 거친 경로의 비율.
$$ P(a_{i,j}|s_i)=\frac{Z_{a_{i,j}}}{Z_{s_i}} $$

**Forward pass**

이제 각 시점별 상태 방문 빈도 값인 $D_{s,t}$를 계산한다

1. $D_{s_i, t} = P(s_i \in \mathcal{S}_I)$.
2. Time horizon만큼 다음을 반복해서 계산한다
    $$ D_{s_k, t+1} = \sum_{s_i}\sum_{a_{i,j}} D_{s_i, t}P(a_{i,j}|s_i)\theta(s_i, a_{i,j}, s_k). $$

**Summing frequencies**

빈도값을 다 더한다: $D_{s_i} = \sum_t D_{s_i, t}$



이제 $D_s$를 사용해서 gradient ascent를 할 수 있다.



#### Maximum Entropy Method의 문제

우리는 사실...

- 환경 모형 $\theta$를 안다고 전제하고 있다. (모르는 경우가 더 많다)
- $R_w=w^\top \phi$로 선형성을 전제하고 있다. (선형이 아닌 경우가 일반적이다)



이 제한조건들도 풀어버릴 수 있을까? → [Inverse Reinforcement Learning (WITHOUT Model)](#inverse-reinforcement-learning-(without-model))



### 기타 방법: Linear Programming

최적화 문제의 대표주자는 선형계획법(LP; Linear Programming)이다.

선형계획법을 해결하는 문제는 이미 연구가 많이 되어 있으니, Imitation Learning을 LP로 변환하는 방법만 알면 편리하게 가져다 쓸 수 있을 것이다.

선형계획법으로 문제를 바꾸기 위해서, $\pi$에 일대일로 대응하는 수치 값이 하나 필요하다.

[Syed et al., 2008][]은 이를 occupancy measure로 정의했다. 이는 각 상태 전환 $(s, a)$을 실행할 확률을 초기 시점 $t=0$에서 합산하기 위해, 할인된(discounted) 합산을 계산한 것이다. 즉, occupancy measure $\rho_\pi$는 다음과 같이 계산된다 (두번째 등호는 [Ho & Ermon, 2016][]에서 차용).
$$
\rho_\pi(s, a) \triangleq \mathbb{E}\left[ \sum_{t=0}^\infty \gamma ^t \mathbb{I}(s_t = s\land a_t=a) \right] = \pi(a|s)\sum_{t=0}^\infty \gamma^t P(s_t | \pi )
$$

- Occupancy measure $\rho$가 결정되면 이에 대응하는 $\pi$도 유일하게 존재하며, 역도 성립한다. 즉, $\rho$를 찾게 된다면 $\pi$를 찾는 것이다. 이 때, $\pi(a|s) = \frac{\rho(a, s)}{\sum_{a'} \rho(a', s)}$.
- 또한, $\rho_\pi$를 알고 있다면, $\rho$의 정의에 따라 $V(\pi) = \sum_{s,a} R_\pi(s,a)\rho_\pi (s,a)$, $V_k(\pi) = \sum_{s, a} \phi_k(s,a) \rho_\pi(s,a)$ 가 성립한다. 다시 말해, MWAL에서 사용한 식 (9)가 $\rho$를 활용해 다시 작성될 수 있다.



#### [Syed et al., 2008][] (LPAL)

[Syed & Schapire, 2007][] (MWAL)을 변형하여 LP로 Imitation Learning을 해결해보자. 식 (6)와 식 (9)를 합치면 다음과 같은 최적화 문제를 얻는다.
$$
\max_{B,\rho}\quad B\quad \textrm{s.t.}\\
B\le \sum_{s,a} \phi_k(s,a) - V_k(\pi_E)\quad \forall k\\
\sum_a \rho(s,a) = P(s_0 = s) + \gamma \sum_{s', a} \rho(s', a)\theta(s, a, s')\\
\rho(s,a) \ge 0
$$
두 번째 행($B$의 제한)은 식 (6)의 변형이며, 세 번째 및 네 번째 행은 식 (9) 및 Bellman flow constraints에서 파생되었다. 이렇게 다음 알고리즘을 얻는다.

1. $\mathcal{T}_E$를 사용해 $V_k(\pi_E)$의 근사치를 계산한다.
2. 식 (17)의 해를 선형계획법으로 푼다.
3. 구해진 해 $\rho$에 대응하는 $\pi$를 반환한다.



이 알고리즘의 오차는 MWAL과 같으며, **반복**이 필요하지 **않고**, **손실함수를 학습하지 않는** 특징이 있다.



------

## Inverse Reinforcement Learning (WITHOUT Model)

*참조: [ICML2018 Tutorial][]*

지금까지 시스템이 스스로 하게 하기 위해서 강화학습을 활용했지만 다음과 같은 문제가 남아있다.

- 환경 모형 $\theta$를 안다고 전제하고 있다. (모르는 경우가 더 많다)
- $R_w=w^\top \phi$로 선형성을 전제하고 있다. (선형이 아닌 경우가 일반적이다)



이제 이 전제를 없애보도록 하자.



<u>[전제조건]</u>

- $S$, $A$, $\pi^*$는 주어진다.
- 환경 모형 $\theta$는 **이제 알지 못한다**.
- 찾고자 하는 보상함수 $R$은 **더 이상 선형이거나 아핀(Affine) 함수가 아니다**. (Deep Learning을 쓰면 된다)



### Sampling: 환경 모형 $\theta$ 를 근사하기

MaxEntIRL을 생각해보자. $\tau$의 확률분포와 보상함수의 관계를 설명하면서 다음과 같은 수식을 사용했다.
$$
P(\tau|w) \propto e^{R_w(\tau)}
$$
확률의 총합은 1이다. 따라서, 
$$
P(\tau|w) = \frac{e^{R_w(\tau)}}{\int e^{R_w(\tau')} d\tau'}
$$
분모의 함수를 $Z(w) := \int e^{R_w(\tau')} d\tau'$ 라고 두자. 여기서 문제는 $Z$를 구하기 위해 모든 $\tau$를 더해야 한다는 데 있다.

게다가 Gradient descent를 사용하려면 $Z$를 구하지 않을 수 없으므로, 방법이 필요하다.
$$
L(w) = \frac{1}{N}\sum_{\tau_i\in \mathcal{T}_E} R_w(\tau_i) + \log Z(w)\\
\nabla L(w) = \frac{1}{N}\sum_{\tau_i\in \mathcal{T}_E}\nabla R_w(\tau_i) + \frac{1}{Z(w)}\nabla Z(w)
$$

#### Sampling?

실제 $\tau$의 분포 $\theta$를 안다면, $\tau$를 $\theta$에서 표본추출함으로써 근사치를 계산할 수 있다. 즉 표본 $S$를 $\theta$에 따라 추출했다면
$$
Z(w) \simeq \sum_{\tau \in S} e^{R_w(\tau)},\quad \nabla Z(w) \simeq \sum_{\tau\in S} R_w(\tau)e^{R_w(\tau)}
$$
이와 같이 계산할 수 있다. 하지만 $\theta$를 구할 수 없다면 $S$를 뽑을 수 없다.



#### Importance Sampling

표본을 추출하기 위한 분포를 모른다면, 적당한 분포를 제안하고 그 분포에 따라 뽑은 다음 효과를 없애면 안될까?

적당한 임의의 확률분포 $q$를 생각하자. 그러면, 다음과 같이 근사할 수 있다.
$$
\int e^{R_w(\tau)}d\tau = \int \frac{e^{R_w(\tau)}}{q(\tau)}q(\tau)d\tau = \mathbb{E}\left[ \frac{e^{R_w(\tau)}}{q(\tau)} \right] \simeq \frac{1}{|S|}\sum_{\tau_i\in S} \frac{e^{R_w(\tau_i)}}{q(\tau_i)}
$$
분포 $q$를 알고 $R_w$는 매 iteration마다 추정값이 있으므로 계산이 가능해졌다. 이제, 다음과 같이 계산할 수 있다. 
$$
L(w) \simeq \frac{1}{N}\sum_{\tau_i\in\mathcal{T}_E} R_w(\tau_i) + \log\frac{1}{|S|}\sum_{\tau_i\in S} \frac{e^{R_w(\tau_i)}}{q(\tau_i)}\\
\nabla L(w) \simeq \frac{1}{N}\sum_{\tau_i\in \mathcal{T}_E}\nabla R_w(\tau_i) + \frac{1}{Z(w)}\sum_{\tau_i\in S} \frac{e^{R_w(\tau_i)}}{q(\tau_i)}\nabla R_w(\tau_i)
$$

#### [Finn et al., 2016][] (GCL; Guided Cost Learning)

한발 더 나아가서, 분포 $q$를 상황에 맞게 갱신해나가고, 분포를 $k$개 사용해서 추출한다면, 다음과 같은 형태의 알고리즘을 얻는다.

1. 모든 $k$에 대해 $q_k(\tau)$를 임의 정책이나 $\pi_E$를 사용하여 $\mathcal{T}$를 구성, 초기화한다.
2. 누적해서 추출된 표본집합 $\mathcal{D}_{samp} = \emptyset$ 라 하자.
3. $m=1$부터 $M$까지,
   1. 각 $q_k(\tau)$에서 표본 $\mathcal{D}_{traj}$를 추출한다.
   2. 표본 업데이트: $\mathcal{D}_{samp} = \mathcal{D}_{samp}\cup \mathcal{D}_{traj}$
   3. $\mathcal{D}_{samp}$와 Gradient descent를 활용해서 $R_w$를 갱신한다: $k=1$부터 $K$까지
      1. $\hat{\mathcal{D}}_{demo}$를 $\mathcal{T}_E$에서 추출한다.
      2. $\hat{\mathcal{D}}_{samp}$를 $\mathcal{D}_{samp}$에서 추출하고, $\hat{\mathcal{D}}_{demo}$를 추가한다.
      3. 식 (6)을 추출된 표본들을 사용해서 계산한다.
      4. $w$를 $\nabla L(w)$를 사용해 갱신한다.
   4. $q_k(\tau)$를 $D_{traj}$와 ([Levine & Abbeel, 2014][])를 사용하여 $q_{k+1}(\tau)$로 갱신한다.
4. $w$와 $q(\tau)$를 반환한다.



다시 보면, 생성된 $\mathcal{D}_{samp}$와 실제 분포인 $\mathcal{T}_E$가 혼합되어 $R_w$를 갱신하는데, $R_w$는 $\mathcal{T}_E$를 더 선호하도록 되어있다.

즉 3.1-3.2, 3.4는 **Generator**, 3.3은 **Discriminator**를 훈련하고 있다.



### Generation: 환경 모형 $\theta$를 생각하지 않기

꼭 환경 모형 $\theta$가 필요한 것일까? 꼭 이런 과정을 거쳐야 하는 것일까?

[Ho & Ermon, 2016][]에 따르면 그렇지도 않은 듯 하다.

* 사실 일부 IRL (식 17)은 $\rho$를 찾는 문제 (식 18)의 dual 문제이다. 즉, 식 18의 해와 식 17의 해는 같다.
  $$
  \arg\max_l\min_\pi -\mathbb{E}_\pi[-\log \pi(a|s)]+ V(\pi) - V(\pi_E) + const.
  $$

  $$
  \min_\rho -\sum_{s,a} \rho(s,a)\log(\pi_\rho (s,a)) \quad\textrm{subject to}\quad \rho(s,a) = \rho_E(s,a)\quad\forall s,a
  $$

  참고로, $w$의 regularizer를 주지 않거나, 상수로 사용하는 경우이다 (const가 regularizer의 위치이다).

* 이전까지의 시도는 $w$의 범위가 매우 제한적이었고, 이는 최적화 문제를 볼록(convex) 집합에서 풀고자 했기 때문이다. 이 때문에 $\pi$는 $\pi_E$에 실질적으로 다가가지 못했다. 물론, $w$의 범위를 제한하면서 상태나 행동의 개수가 많은 경우를 policy approximation 문제로 풀 수 있게 되기는 했다.

$w$의 범위를 제한하는 대신에, 손실함수 $l$이 일정 수준 이상이 되면 penalty를 부과하고, $\pi_E$와 다른 $\pi$를 멀리 띄워보도록 하자. 즉, 수식 17을 다음과 같이 생각하기로 한다 (regularizer $\psi$가 추가되었다).
$$
\arg\max_l\min_\pi -\mathbb{E}_\pi[-\log \pi(a|s)]+ V(\pi) - V(\pi_E) + \psi(l)\\
\textrm{where}\quad \psi(l)\triangleq \left\{\begin{array}{ll}\mathbb{E}_{\pi_E}[g(l(s, a))]&\textrm{if } l(s,a) < 0\\ +\infty &\textrm{otherwise} \end{array}\right.\\
g(x) = \left\{\begin{array}{ll} -x-\log(1-e^x)&\textrm{if }x <0\\+\infty&\textrm{otherwise} \end{array}\right.
$$
이 복잡한 수식은 Dual 문제를 간단히 하기 위한 것으로 사실 다음 함수의 convex conjugate이다.
$$
\psi^* (\rho -\rho_E) = \max_{D} \mathbb{E}_\pi[\log D(s,a)] + \mathbb{E}_{\pi_E}[\log(1-D(s,a))]
$$
이 때, $D$는 손실함수를 $\rho$와 $\rho_E$를 구분하는 문제로 치환한 결과이다. 이를 이용한 dual 문제의 해는 Jensen-Shannon 발산 함수가 된다.

따라서 다음과 같은 알고리즘을 얻는다.



#### [Ho & Ermon, 2016][] (GAIL; Generative Adversarial Imitation Learning)

1. 초기 정책 $\pi_0$를 초기화한다. 즉, 정책 네트워크의 매개변수 $w_p$를 초기화한다.

2. $\pi_E$와 다른 $\pi$를 구분하기 위한 Discriminator 네트워크 $D_{w_d}:  \mathcal{S}\times \mathcal{A}\to (0,1)$를  초기화한다. 즉, $w_d$를 초기화한다.

3. $m=1$부터 증가시키면서 다음을 반복한다:

   1. $\mathcal{T}(\pi_m)$을 표본추출한다.

   2. $w_d$를 다음 그래디언트 값을 사용해 갱신한다. (논문에서는 Adam을 사용했다)
      $$
      \hat{\mathbb{E}}_{\mathcal{T}(\pi_m)}[\nabla_{w_d} \log D_{w_d} (s,a)] + \hat{\mathbb{E}}_{\mathcal{T}_E}[\nabla_{w_d} \log(1-D_{w_d}(s,a))]
      $$

   3. 다음 그래디언트 값을 사용해서 $w_p$를 갱신한다. 
      (KL-제한조건 하; $Q(s,a) \triangleq \hat{\mathbb{E}}_{\mathcal{T}(\pi_m)}[\log D_{w_d}(s',a')|s_0 = s, a_0=a]$)
      $$
      \hat{\mathbb{E}}_{\mathcal{T}(\pi_m)}[\nabla_{w_p} \log \pi_m (a|s) Q(s,a)] - \lambda \nabla_{w_p}(-\mathbb{E}_{\pi_m}[-\log\pi_m(a|s)])
      $$






단계 3에 대한 자세한 사항은 TRPO 논문 ([Schulman et al., 2015][]) 참고.



## TODO

- TRPO
- Value Iteration Network



------

# 참고문헌

## 웹 페이지들

[Wiki:Agent]: https://ko.wikipedia.org/wiki/지능형_에이전트

## 학회 튜토리얼 또는 수업 자료들

[CMU10703]: http://www.andrew.cmu.edu/course/10-703/	"Deep Reinforcement Learning and Control (Retrieved at 18.11.30)"
[EACL2017 Tutorial]: https://github.com/sheffieldnlp/ImitationLearningTutorialEACL2017	" EACL2017 Tutorial on Imitation Learning "
[ICML2018 Tutorial]: https://sites.google.com/view/icml2018-imitation-learning/	" Imitation Learning Tutorial (Retrieved at 18.11.30) "

## 책

[Sutton & Barto, 2017]: http://incompleteideas.net/book/bookdraft2017nov5.pdf	"Sutton & Barto. (2017). Reinforcement Learning: An Introduction. 2nd ed."

## 논문들

[Abbeel & Ng, 2004]: http://doi.acm.org/10.1145/1015330.1015430	" Abbeel & Ng. (2004). Apprenticeship Learning via Inverse Reinforcement Learning. ICML '04 "
[Clark & Manning, 2015]: https://cs.stanford.edu/people/kevclark/resources/clark-manning-acl15-entity.pdf "Clark & Manning. (2015). Entity-Centric Coreference Resolution with Model Stacking. ACL '15"
[Daume et al., 2009]: https://link.springer.com/article/10.1007/s10994-009-5106-x	"Daume et al. (2009). Search-based structured prediction. Machine Learning 75"
[Finn et al., 2016]: https://arxiv.org/abs/1603.00448	"Finn et al. (2016). Guided Cost Learning: Deep Inverse Optimal Control via Policy Optimization. ICML '16"
[Ho & Ermon, 2016]: https://arxiv.org/pdf/1606.03476.pdf	"Ho & Ermon. (2016). Generative adversarial imitation learning. NIPS '16"
[Ratliff et al., 2006]: http://doi.acm.org/10.1145/1143844.1143936	" Ratliff, Bagnell, & Zinkevich. (2006). Maximum Margin Planning. ICML '06 "
[Ross & Bagnell, 2010]: http://proceedings.mlr.press/v9/ross10a/ross10a.pdf	"Ross & Bagnell. (2010). Efficient Reductions for Imitation Learning. AISTATS '10"
[Ross et al., 2011]: http://proceedings.mlr.press/v15/ross11a/ross11a.pdf	"Ross et al. (2011). A Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning. AISTATS '11"
[Syed & Schapire, 2007]: https://papers.nips.cc/paper/3293-a-game-theoretic-approach-to-apprenticeship-learning.pdf	"Syed & Schapire. (2007). A Game-Theoretic Approach to Apprenticeship Learning. NIPS '07"
[Syed et al., 2008]: http://rob.schapire.net/papers/SyedBowlingSchapireICML2008.pdf	"Syed et al. (2008). Apprenticeship learning using linear programming. ICML '08"
[Ziebart et al., 2008]: https://dl.acm.org/citation.cfm?id=1620297	" Ziebart et al. (2008). Maximum entropy inverse reinforcement learning. AAAI '08 "

## 아직 정리되지 않은 자료들

[Schulman et al., 2015]: https://arxiv.org/pdf/1502.05477.pdf	"Schulman et al. (2015). Trust Region Policy Optimization. ICML '15"
[Tamar et al., 2016]: https://arxiv.org/pdf/1602.02867.pdf	"Tamar et al. (2016). Value Iteration Networks. NIPS '16"

